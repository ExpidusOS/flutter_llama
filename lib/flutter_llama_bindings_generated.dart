// ignore_for_file: always_specify_types
// ignore_for_file: camel_case_types
// ignore_for_file: non_constant_identifier_names

// AUTO GENERATED FILE, DO NOT EDIT.
//
// Generated by `package:ffigen`.
// ignore_for_file: type=lint
import 'dart:ffi' as ffi;

/// Bindings for `src/llama.cpp/llama.h`.
///
/// Regenerate bindings with `flutter pub run ffigen --config ffigen.yaml`.
///
class FlutterLlamaBindings {
  /// Holds the symbol lookup function.
  final ffi.Pointer<T> Function<T extends ffi.NativeType>(String symbolName)
      _lookup;

  /// The symbols are looked up in [dynamicLibrary].
  FlutterLlamaBindings(ffi.DynamicLibrary dynamicLibrary)
      : _lookup = dynamicLibrary.lookup;

  /// The symbols are looked up with [lookup].
  FlutterLlamaBindings.fromLookup(
      ffi.Pointer<T> Function<T extends ffi.NativeType>(String symbolName)
          lookup)
      : _lookup = lookup;

  context_params context_default_params() {
    return _context_default_params();
  }

  late final _context_default_paramsPtr =
      _lookup<ffi.NativeFunction<context_params Function()>>(
          'llama_context_default_params');
  late final _context_default_params =
      _context_default_paramsPtr.asFunction<context_params Function()>();

  model_quantize_params model_quantize_default_params() {
    return _model_quantize_default_params();
  }

  late final _model_quantize_default_paramsPtr =
      _lookup<ffi.NativeFunction<model_quantize_params Function()>>(
          'llama_model_quantize_default_params');
  late final _model_quantize_default_params = _model_quantize_default_paramsPtr
      .asFunction<model_quantize_params Function()>();

  /// Initialize the llama + ggml backend
  /// If numa is true, use NUMA optimizations
  /// Call once at the start of the program
  void backend_init(
    bool numa,
  ) {
    return _backend_init(
      numa,
    );
  }

  late final _backend_initPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Bool)>>(
          'llama_backend_init');
  late final _backend_init = _backend_initPtr.asFunction<void Function(bool)>();

  /// Call once at the end of the program - currently only used for MPI
  void backend_free() {
    return _backend_free();
  }

  late final _backend_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function()>>('llama_backend_free');
  late final _backend_free = _backend_freePtr.asFunction<void Function()>();

  ffi.Pointer<model> load_model_from_file(
    ffi.Pointer<ffi.Char> path_model,
    context_params params,
  ) {
    return _load_model_from_file(
      path_model,
      params,
    );
  }

  late final _load_model_from_filePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<model> Function(ffi.Pointer<ffi.Char>,
              context_params)>>('llama_load_model_from_file');
  late final _load_model_from_file = _load_model_from_filePtr.asFunction<
      ffi.Pointer<model> Function(ffi.Pointer<ffi.Char>, context_params)>();

  void free_model(
    ffi.Pointer<model> model,
  ) {
    return _free_model(
      model,
    );
  }

  late final _free_modelPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<model>)>>(
          'llama_free_model');
  late final _free_model =
      _free_modelPtr.asFunction<void Function(ffi.Pointer<model>)>();

  ffi.Pointer<context> new_context_with_model(
    ffi.Pointer<model> model,
    context_params params,
  ) {
    return _new_context_with_model(
      model,
      params,
    );
  }

  late final _new_context_with_modelPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<context> Function(ffi.Pointer<model>,
              context_params)>>('llama_new_context_with_model');
  late final _new_context_with_model = _new_context_with_modelPtr.asFunction<
      ffi.Pointer<context> Function(ffi.Pointer<model>, context_params)>();

  /// Frees all allocated memory
  void free(
    ffi.Pointer<context> ctx,
  ) {
    return _free(
      ctx,
    );
  }

  late final _freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<context>)>>(
          'llama_free');
  late final _free = _freePtr.asFunction<void Function(ffi.Pointer<context>)>();

  int time_us() {
    return _time_us();
  }

  late final _time_usPtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function()>>('llama_time_us');
  late final _time_us = _time_usPtr.asFunction<int Function()>();

  int max_devices() {
    return _max_devices();
  }

  late final _max_devicesPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('llama_max_devices');
  late final _max_devices = _max_devicesPtr.asFunction<int Function()>();

  bool mmap_supported() {
    return _mmap_supported();
  }

  late final _mmap_supportedPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function()>>('llama_mmap_supported');
  late final _mmap_supported = _mmap_supportedPtr.asFunction<bool Function()>();

  bool mlock_supported() {
    return _mlock_supported();
  }

  late final _mlock_supportedPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function()>>('llama_mlock_supported');
  late final _mlock_supported =
      _mlock_supportedPtr.asFunction<bool Function()>();

  int n_vocab(
    ffi.Pointer<context> ctx,
  ) {
    return _n_vocab(
      ctx,
    );
  }

  late final _n_vocabPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<context>)>>(
          'llama_n_vocab');
  late final _n_vocab =
      _n_vocabPtr.asFunction<int Function(ffi.Pointer<context>)>();

  int n_ctx(
    ffi.Pointer<context> ctx,
  ) {
    return _n_ctx(
      ctx,
    );
  }

  late final _n_ctxPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<context>)>>(
          'llama_n_ctx');
  late final _n_ctx =
      _n_ctxPtr.asFunction<int Function(ffi.Pointer<context>)>();

  int n_ctx_train(
    ffi.Pointer<context> ctx,
  ) {
    return _n_ctx_train(
      ctx,
    );
  }

  late final _n_ctx_trainPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<context>)>>(
          'llama_n_ctx_train');
  late final _n_ctx_train =
      _n_ctx_trainPtr.asFunction<int Function(ffi.Pointer<context>)>();

  int n_embd(
    ffi.Pointer<context> ctx,
  ) {
    return _n_embd(
      ctx,
    );
  }

  late final _n_embdPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<context>)>>(
          'llama_n_embd');
  late final _n_embd =
      _n_embdPtr.asFunction<int Function(ffi.Pointer<context>)>();

  int vocab_type1(
    ffi.Pointer<context> ctx,
  ) {
    return _vocab_type1(
      ctx,
    );
  }

  late final _vocab_type1Ptr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<context>)>>(
          'llama_vocab_type');
  late final _vocab_type1 =
      _vocab_type1Ptr.asFunction<int Function(ffi.Pointer<context>)>();

  int model_n_vocab(
    ffi.Pointer<model> model,
  ) {
    return _model_n_vocab(
      model,
    );
  }

  late final _model_n_vocabPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<model>)>>(
          'llama_model_n_vocab');
  late final _model_n_vocab =
      _model_n_vocabPtr.asFunction<int Function(ffi.Pointer<model>)>();

  int model_n_ctx(
    ffi.Pointer<model> model,
  ) {
    return _model_n_ctx(
      model,
    );
  }

  late final _model_n_ctxPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<model>)>>(
          'llama_model_n_ctx');
  late final _model_n_ctx =
      _model_n_ctxPtr.asFunction<int Function(ffi.Pointer<model>)>();

  int model_n_ctx_train(
    ffi.Pointer<model> model,
  ) {
    return _model_n_ctx_train(
      model,
    );
  }

  late final _model_n_ctx_trainPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<model>)>>(
          'llama_model_n_ctx_train');
  late final _model_n_ctx_train =
      _model_n_ctx_trainPtr.asFunction<int Function(ffi.Pointer<model>)>();

  int model_n_embd(
    ffi.Pointer<model> model,
  ) {
    return _model_n_embd(
      model,
    );
  }

  late final _model_n_embdPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<model>)>>(
          'llama_model_n_embd');
  late final _model_n_embd =
      _model_n_embdPtr.asFunction<int Function(ffi.Pointer<model>)>();

  /// Get a string describing the model type
  int model_desc(
    ffi.Pointer<model> model,
    ffi.Pointer<ffi.Char> buf,
    int buf_size,
  ) {
    return _model_desc(
      model,
      buf,
      buf_size,
    );
  }

  late final _model_descPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int Function(ffi.Pointer<model>, ffi.Pointer<ffi.Char>,
              ffi.Size)>>('llama_model_desc');
  late final _model_desc = _model_descPtr.asFunction<
      int Function(ffi.Pointer<model>, ffi.Pointer<ffi.Char>, int)>();

  /// Returns the total size of all the tensors in the model in bytes
  int model_size(
    ffi.Pointer<model> model,
  ) {
    return _model_size(
      model,
    );
  }

  late final _model_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Uint64 Function(ffi.Pointer<model>)>>(
          'llama_model_size');
  late final _model_size =
      _model_sizePtr.asFunction<int Function(ffi.Pointer<model>)>();

  /// Returns the total number of parameters in the model
  int model_n_params(
    ffi.Pointer<model> model,
  ) {
    return _model_n_params(
      model,
    );
  }

  late final _model_n_paramsPtr =
      _lookup<ffi.NativeFunction<ffi.Uint64 Function(ffi.Pointer<model>)>>(
          'llama_model_n_params');
  late final _model_n_params =
      _model_n_paramsPtr.asFunction<int Function(ffi.Pointer<model>)>();

  /// Returns 0 on success
  int model_quantize(
    ffi.Pointer<ffi.Char> fname_inp,
    ffi.Pointer<ffi.Char> fname_out,
    ffi.Pointer<model_quantize_params> params,
  ) {
    return _model_quantize(
      fname_inp,
      fname_out,
      params,
    );
  }

  late final _model_quantizePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>,
              ffi.Pointer<model_quantize_params>)>>('llama_model_quantize');
  late final _model_quantize = _model_quantizePtr.asFunction<
      int Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>,
          ffi.Pointer<model_quantize_params>)>();

  int apply_lora_from_file(
    ffi.Pointer<context> ctx,
    ffi.Pointer<ffi.Char> path_lora,
    ffi.Pointer<ffi.Char> path_base_model,
    int n_threads,
  ) {
    return _apply_lora_from_file(
      ctx,
      path_lora,
      path_base_model,
      n_threads,
    );
  }

  late final _apply_lora_from_filePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int Function(ffi.Pointer<context>, ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Char>, ffi.Int)>>('llama_apply_lora_from_file');
  late final _apply_lora_from_file = _apply_lora_from_filePtr.asFunction<
      int Function(ffi.Pointer<context>, ffi.Pointer<ffi.Char>,
          ffi.Pointer<ffi.Char>, int)>();

  int model_apply_lora_from_file(
    ffi.Pointer<model> model,
    ffi.Pointer<ffi.Char> path_lora,
    ffi.Pointer<ffi.Char> path_base_model,
    int n_threads,
  ) {
    return _model_apply_lora_from_file(
      model,
      path_lora,
      path_base_model,
      n_threads,
    );
  }

  late final _model_apply_lora_from_filePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int Function(
              ffi.Pointer<model>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Char>,
              ffi.Int)>>('llama_model_apply_lora_from_file');
  late final _model_apply_lora_from_file =
      _model_apply_lora_from_filePtr.asFunction<
          int Function(ffi.Pointer<model>, ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Char>, int)>();

  /// Returns the number of tokens in the KV cache
  int get_kv_cache_token_count(
    ffi.Pointer<context> ctx,
  ) {
    return _get_kv_cache_token_count(
      ctx,
    );
  }

  late final _get_kv_cache_token_countPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<context>)>>(
          'llama_get_kv_cache_token_count');
  late final _get_kv_cache_token_count = _get_kv_cache_token_countPtr
      .asFunction<int Function(ffi.Pointer<context>)>();

  /// Sets the current rng seed.
  void set_rng_seed(
    ffi.Pointer<context> ctx,
    int seed,
  ) {
    return _set_rng_seed(
      ctx,
      seed,
    );
  }

  late final _set_rng_seedPtr = _lookup<
          ffi
          .NativeFunction<ffi.Void Function(ffi.Pointer<context>, ffi.Uint32)>>(
      'llama_set_rng_seed');
  late final _set_rng_seed =
      _set_rng_seedPtr.asFunction<void Function(ffi.Pointer<context>, int)>();

  /// Returns the maximum size in bytes of the state (rng, logits, embedding
  /// and kv_cache) - will often be smaller after compacting tokens
  int get_state_size(
    ffi.Pointer<context> ctx,
  ) {
    return _get_state_size(
      ctx,
    );
  }

  late final _get_state_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<context>)>>(
          'llama_get_state_size');
  late final _get_state_size =
      _get_state_sizePtr.asFunction<int Function(ffi.Pointer<context>)>();

  /// Copies the state to the specified destination address.
  /// Destination needs to have allocated enough memory.
  /// Returns the number of bytes copied
  int copy_state_data(
    ffi.Pointer<context> ctx,
    ffi.Pointer<ffi.Uint8> dst,
  ) {
    return _copy_state_data(
      ctx,
      dst,
    );
  }

  late final _copy_state_dataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Size Function(ffi.Pointer<context>,
              ffi.Pointer<ffi.Uint8>)>>('llama_copy_state_data');
  late final _copy_state_data = _copy_state_dataPtr
      .asFunction<int Function(ffi.Pointer<context>, ffi.Pointer<ffi.Uint8>)>();

  /// Set the state reading from the specified address
  /// Returns the number of bytes read
  int set_state_data(
    ffi.Pointer<context> ctx,
    ffi.Pointer<ffi.Uint8> src,
  ) {
    return _set_state_data(
      ctx,
      src,
    );
  }

  late final _set_state_dataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Size Function(ffi.Pointer<context>,
              ffi.Pointer<ffi.Uint8>)>>('llama_set_state_data');
  late final _set_state_data = _set_state_dataPtr
      .asFunction<int Function(ffi.Pointer<context>, ffi.Pointer<ffi.Uint8>)>();

  /// Save/load session file
  bool load_session_file(
    ffi.Pointer<context> ctx,
    ffi.Pointer<ffi.Char> path_session,
    ffi.Pointer<token> tokens_out,
    int n_token_capacity,
    ffi.Pointer<ffi.Size> n_token_count_out,
  ) {
    return _load_session_file(
      ctx,
      path_session,
      tokens_out,
      n_token_capacity,
      n_token_count_out,
    );
  }

  late final _load_session_filePtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(
              ffi.Pointer<context>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<token>,
              ffi.Size,
              ffi.Pointer<ffi.Size>)>>('llama_load_session_file');
  late final _load_session_file = _load_session_filePtr.asFunction<
      bool Function(ffi.Pointer<context>, ffi.Pointer<ffi.Char>,
          ffi.Pointer<token>, int, ffi.Pointer<ffi.Size>)>();

  bool save_session_file(
    ffi.Pointer<context> ctx,
    ffi.Pointer<ffi.Char> path_session,
    ffi.Pointer<token> tokens,
    int n_token_count,
  ) {
    return _save_session_file(
      ctx,
      path_session,
      tokens,
      n_token_count,
    );
  }

  late final _save_session_filePtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<context>, ffi.Pointer<ffi.Char>,
              ffi.Pointer<token>, ffi.Size)>>('llama_save_session_file');
  late final _save_session_file = _save_session_filePtr.asFunction<
      bool Function(ffi.Pointer<context>, ffi.Pointer<ffi.Char>,
          ffi.Pointer<token>, int)>();

  /// Run the llama inference to obtain the logits and probabilities for the next token.
  /// tokens + n_tokens is the provided batch of new tokens to process
  /// n_past is the number of tokens to use from previous eval calls
  /// Returns 0 on success
  int eval(
    ffi.Pointer<context> ctx,
    ffi.Pointer<token> tokens,
    int n_tokens,
    int n_past,
    int n_threads,
  ) {
    return _eval(
      ctx,
      tokens,
      n_tokens,
      n_past,
      n_threads,
    );
  }

  late final _evalPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int Function(ffi.Pointer<context>, ffi.Pointer<token>, ffi.Int,
              ffi.Int, ffi.Int)>>('llama_eval');
  late final _eval = _evalPtr.asFunction<
      int Function(ffi.Pointer<context>, ffi.Pointer<token>, int, int, int)>();

  /// Same as llama_eval, but use float matrix input directly.
  int eval_embd(
    ffi.Pointer<context> ctx,
    ffi.Pointer<ffi.Float> embd,
    int n_tokens,
    int n_past,
    int n_threads,
  ) {
    return _eval_embd(
      ctx,
      embd,
      n_tokens,
      n_past,
      n_threads,
    );
  }

  late final _eval_embdPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int Function(ffi.Pointer<context>, ffi.Pointer<ffi.Float>,
              ffi.Int, ffi.Int, ffi.Int)>>('llama_eval_embd');
  late final _eval_embd = _eval_embdPtr.asFunction<
      int Function(
          ffi.Pointer<context>, ffi.Pointer<ffi.Float>, int, int, int)>();

  /// Export a static computation graph for context of 511 and batch size of 1
  /// NOTE: since this functionality is mostly for debugging and demonstration purposes, we hardcode these
  /// parameters here to keep things simple
  /// IMPORTANT: do not use for anything else other than debugging and testing!
  int eval_export(
    ffi.Pointer<context> ctx,
    ffi.Pointer<ffi.Char> fname,
  ) {
    return _eval_export(
      ctx,
      fname,
    );
  }

  late final _eval_exportPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int Function(ffi.Pointer<context>,
              ffi.Pointer<ffi.Char>)>>('llama_eval_export');
  late final _eval_export = _eval_exportPtr
      .asFunction<int Function(ffi.Pointer<context>, ffi.Pointer<ffi.Char>)>();

  /// Token logits obtained from the last call to llama_eval()
  /// The logits for the last token are stored in the last row
  /// Can be mutated in order to change the probabilities of the next token
  /// Rows: n_tokens
  /// Cols: n_vocab
  ffi.Pointer<ffi.Float> get_logits(
    ffi.Pointer<context> ctx,
  ) {
    return _get_logits(
      ctx,
    );
  }

  late final _get_logitsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(
              ffi.Pointer<context>)>>('llama_get_logits');
  late final _get_logits = _get_logitsPtr
      .asFunction<ffi.Pointer<ffi.Float> Function(ffi.Pointer<context>)>();

  /// Get the embeddings for the input
  /// shape: [n_embd] (1-dimensional)
  ffi.Pointer<ffi.Float> get_embeddings(
    ffi.Pointer<context> ctx,
  ) {
    return _get_embeddings(
      ctx,
    );
  }

  late final _get_embeddingsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(
              ffi.Pointer<context>)>>('llama_get_embeddings');
  late final _get_embeddings = _get_embeddingsPtr
      .asFunction<ffi.Pointer<ffi.Float> Function(ffi.Pointer<context>)>();

  /// Vocab
  ffi.Pointer<ffi.Char> token_get_text(
    ffi.Pointer<context> ctx,
    int token,
  ) {
    return _token_get_text(
      ctx,
      token,
    );
  }

  late final _token_get_textPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(
              ffi.Pointer<context>, token)>>('llama_token_get_text');
  late final _token_get_text = _token_get_textPtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ffi.Pointer<context>, int)>();

  double token_get_score(
    ffi.Pointer<context> ctx,
    int token,
  ) {
    return _token_get_score(
      ctx,
      token,
    );
  }

  late final _token_get_scorePtr = _lookup<
          ffi.NativeFunction<ffi.Float Function(ffi.Pointer<context>, token)>>(
      'llama_token_get_score');
  late final _token_get_score = _token_get_scorePtr
      .asFunction<double Function(ffi.Pointer<context>, int)>();

  int token_get_type(
    ffi.Pointer<context> ctx,
    int token,
  ) {
    return _token_get_type(
      ctx,
      token,
    );
  }

  late final _token_get_typePtr = _lookup<
          ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<context>, token)>>(
      'llama_token_get_type');
  late final _token_get_type =
      _token_get_typePtr.asFunction<int Function(ffi.Pointer<context>, int)>();

  /// Special tokens
  int token_bos(
    ffi.Pointer<context> ctx,
  ) {
    return _token_bos(
      ctx,
    );
  }

  late final _token_bosPtr =
      _lookup<ffi.NativeFunction<token Function(ffi.Pointer<context>)>>(
          'llama_token_bos');
  late final _token_bos =
      _token_bosPtr.asFunction<int Function(ffi.Pointer<context>)>();

  int token_eos(
    ffi.Pointer<context> ctx,
  ) {
    return _token_eos(
      ctx,
    );
  }

  late final _token_eosPtr =
      _lookup<ffi.NativeFunction<token Function(ffi.Pointer<context>)>>(
          'llama_token_eos');
  late final _token_eos =
      _token_eosPtr.asFunction<int Function(ffi.Pointer<context>)>();

  int token_nl(
    ffi.Pointer<context> ctx,
  ) {
    return _token_nl(
      ctx,
    );
  }

  late final _token_nlPtr =
      _lookup<ffi.NativeFunction<token Function(ffi.Pointer<context>)>>(
          'llama_token_nl');
  late final _token_nl =
      _token_nlPtr.asFunction<int Function(ffi.Pointer<context>)>();

  /// Convert the provided text into tokens.
  /// The tokens pointer must be large enough to hold the resulting tokens.
  /// Returns the number of tokens on success, no more than n_max_tokens
  /// Returns a negative number on failure - the number of tokens that would have been returned
  int tokenize(
    ffi.Pointer<context> ctx,
    ffi.Pointer<ffi.Char> text,
    ffi.Pointer<token> tokens,
    int n_max_tokens,
    bool add_bos,
  ) {
    return _tokenize(
      ctx,
      text,
      tokens,
      n_max_tokens,
      add_bos,
    );
  }

  late final _tokenizePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int Function(ffi.Pointer<context>, ffi.Pointer<ffi.Char>,
              ffi.Pointer<token>, ffi.Int, ffi.Bool)>>('llama_tokenize');
  late final _tokenize = _tokenizePtr.asFunction<
      int Function(ffi.Pointer<context>, ffi.Pointer<ffi.Char>,
          ffi.Pointer<token>, int, bool)>();

  int tokenize_with_model(
    ffi.Pointer<model> model,
    ffi.Pointer<ffi.Char> text,
    ffi.Pointer<token> tokens,
    int n_max_tokens,
    bool add_bos,
  ) {
    return _tokenize_with_model(
      model,
      text,
      tokens,
      n_max_tokens,
      add_bos,
    );
  }

  late final _tokenize_with_modelPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int Function(
              ffi.Pointer<model>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<token>,
              ffi.Int,
              ffi.Bool)>>('llama_tokenize_with_model');
  late final _tokenize_with_model = _tokenize_with_modelPtr.asFunction<
      int Function(ffi.Pointer<model>, ffi.Pointer<ffi.Char>,
          ffi.Pointer<token>, int, bool)>();

  /// Token Id -> Piece.
  /// Uses the vocabulary in the provided context.
  /// Does not write null terminator to the buffer.
  /// User code is responsible to remove the leading whitespace of the first non-BOS token when decoding multiple tokens.
  int token_to_piece(
    ffi.Pointer<context> ctx,
    int token,
    ffi.Pointer<ffi.Char> buf,
    int length,
  ) {
    return _token_to_piece(
      ctx,
      token,
      buf,
      length,
    );
  }

  late final _token_to_piecePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int Function(ffi.Pointer<context>, token, ffi.Pointer<ffi.Char>,
              ffi.Int)>>('llama_token_to_piece');
  late final _token_to_piece = _token_to_piecePtr.asFunction<
      int Function(ffi.Pointer<context>, int, ffi.Pointer<ffi.Char>, int)>();

  int token_to_piece_with_model(
    ffi.Pointer<model> model,
    int token,
    ffi.Pointer<ffi.Char> buf,
    int length,
  ) {
    return _token_to_piece_with_model(
      model,
      token,
      buf,
      length,
    );
  }

  late final _token_to_piece_with_modelPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int Function(ffi.Pointer<model>, token, ffi.Pointer<ffi.Char>,
              ffi.Int)>>('llama_token_to_piece_with_model');
  late final _token_to_piece_with_model =
      _token_to_piece_with_modelPtr.asFunction<
          int Function(ffi.Pointer<model>, int, ffi.Pointer<ffi.Char>, int)>();

  /// Grammar
  ffi.Pointer<grammar> grammar_init(
    ffi.Pointer<ffi.Pointer<grammar_element>> rules,
    int n_rules,
    int start_rule_index,
  ) {
    return _grammar_init(
      rules,
      n_rules,
      start_rule_index,
    );
  }

  late final _grammar_initPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<grammar> Function(
              ffi.Pointer<ffi.Pointer<grammar_element>>,
              ffi.Size,
              ffi.Size)>>('llama_grammar_init');
  late final _grammar_init = _grammar_initPtr.asFunction<
      ffi.Pointer<grammar> Function(
          ffi.Pointer<ffi.Pointer<grammar_element>>, int, int)>();

  void grammar_free(
    ffi.Pointer<grammar> grammar,
  ) {
    return _grammar_free(
      grammar,
    );
  }

  late final _grammar_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<grammar>)>>(
          'llama_grammar_free');
  late final _grammar_free =
      _grammar_freePtr.asFunction<void Function(ffi.Pointer<grammar>)>();

  ffi.Pointer<grammar> grammar_copy(
    ffi.Pointer<grammar> grammar,
  ) {
    return _grammar_copy(
      grammar,
    );
  }

  late final _grammar_copyPtr = _lookup<
          ffi
          .NativeFunction<ffi.Pointer<grammar> Function(ffi.Pointer<grammar>)>>(
      'llama_grammar_copy');
  late final _grammar_copy = _grammar_copyPtr
      .asFunction<ffi.Pointer<grammar> Function(ffi.Pointer<grammar>)>();

  /// @details Repetition penalty described in CTRL academic paper https://arxiv.org/abs/1909.05858, with negative logit fix.
  void sample_repetition_penalty(
    ffi.Pointer<context> ctx,
    ffi.Pointer<token_data_array> candidates,
    ffi.Pointer<token> last_tokens,
    int last_tokens_size,
    double penalty,
  ) {
    return _sample_repetition_penalty(
      ctx,
      candidates,
      last_tokens,
      last_tokens_size,
      penalty,
    );
  }

  late final _sample_repetition_penaltyPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<context>,
              ffi.Pointer<token_data_array>,
              ffi.Pointer<token>,
              ffi.Size,
              ffi.Float)>>('llama_sample_repetition_penalty');
  late final _sample_repetition_penalty =
      _sample_repetition_penaltyPtr.asFunction<
          void Function(ffi.Pointer<context>, ffi.Pointer<token_data_array>,
              ffi.Pointer<token>, int, double)>();

  /// @details Frequency and presence penalties described in OpenAI API https://platform.openai.com/docs/api-reference/parameter-details.
  void sample_frequency_and_presence_penalties(
    ffi.Pointer<context> ctx,
    ffi.Pointer<token_data_array> candidates,
    ffi.Pointer<token> last_tokens,
    int last_tokens_size,
    double alpha_frequency,
    double alpha_presence,
  ) {
    return _sample_frequency_and_presence_penalties(
      ctx,
      candidates,
      last_tokens,
      last_tokens_size,
      alpha_frequency,
      alpha_presence,
    );
  }

  late final _sample_frequency_and_presence_penaltiesPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<context>,
              ffi.Pointer<token_data_array>,
              ffi.Pointer<token>,
              ffi.Size,
              ffi.Float,
              ffi.Float)>>('llama_sample_frequency_and_presence_penalties');
  late final _sample_frequency_and_presence_penalties =
      _sample_frequency_and_presence_penaltiesPtr.asFunction<
          void Function(ffi.Pointer<context>, ffi.Pointer<token_data_array>,
              ffi.Pointer<token>, int, double, double)>();

  /// @details Apply classifier-free guidance to the logits as described in academic paper "Stay on topic with Classifier-Free Guidance" https://arxiv.org/abs/2306.17806
  /// @param candidates A vector of `llama_token_data` containing the candidate tokens, the logits must be directly extracted from the original generation context without being sorted.
  /// @params guidance_ctx A separate context from the same model. Other than a negative prompt at the beginning, it should have all generated and user input tokens copied from the main context.
  /// @params scale Guidance strength. 1.0f means no guidance. Higher values mean stronger guidance.
  void sample_classifier_free_guidance(
    ffi.Pointer<context> ctx,
    ffi.Pointer<token_data_array> candidates,
    ffi.Pointer<context> guidance_ctx,
    double scale,
  ) {
    return _sample_classifier_free_guidance(
      ctx,
      candidates,
      guidance_ctx,
      scale,
    );
  }

  late final _sample_classifier_free_guidancePtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<context>,
              ffi.Pointer<token_data_array>,
              ffi.Pointer<context>,
              ffi.Float)>>('llama_sample_classifier_free_guidance');
  late final _sample_classifier_free_guidance =
      _sample_classifier_free_guidancePtr.asFunction<
          void Function(ffi.Pointer<context>, ffi.Pointer<token_data_array>,
              ffi.Pointer<context>, double)>();

  /// @details Sorts candidate tokens by their logits in descending order and calculate probabilities based on logits.
  void sample_softmax(
    ffi.Pointer<context> ctx,
    ffi.Pointer<token_data_array> candidates,
  ) {
    return _sample_softmax(
      ctx,
      candidates,
    );
  }

  late final _sample_softmaxPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<context>,
              ffi.Pointer<token_data_array>)>>('llama_sample_softmax');
  late final _sample_softmax = _sample_softmaxPtr.asFunction<
      void Function(ffi.Pointer<context>, ffi.Pointer<token_data_array>)>();

  /// @details Top-K sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
  void sample_top_k(
    ffi.Pointer<context> ctx,
    ffi.Pointer<token_data_array> candidates,
    int k,
    int min_keep,
  ) {
    return _sample_top_k(
      ctx,
      candidates,
      k,
      min_keep,
    );
  }

  late final _sample_top_kPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<context>, ffi.Pointer<token_data_array>,
              ffi.Int, ffi.Size)>>('llama_sample_top_k');
  late final _sample_top_k = _sample_top_kPtr.asFunction<
      void Function(
          ffi.Pointer<context>, ffi.Pointer<token_data_array>, int, int)>();

  /// @details Nucleus sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
  void sample_top_p(
    ffi.Pointer<context> ctx,
    ffi.Pointer<token_data_array> candidates,
    double p,
    int min_keep,
  ) {
    return _sample_top_p(
      ctx,
      candidates,
      p,
      min_keep,
    );
  }

  late final _sample_top_pPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<context>, ffi.Pointer<token_data_array>,
              ffi.Float, ffi.Size)>>('llama_sample_top_p');
  late final _sample_top_p = _sample_top_pPtr.asFunction<
      void Function(
          ffi.Pointer<context>, ffi.Pointer<token_data_array>, double, int)>();

  /// @details Tail Free Sampling described in https://www.trentonbricken.com/Tail-Free-Sampling/.
  void sample_tail_free(
    ffi.Pointer<context> ctx,
    ffi.Pointer<token_data_array> candidates,
    double z,
    int min_keep,
  ) {
    return _sample_tail_free(
      ctx,
      candidates,
      z,
      min_keep,
    );
  }

  late final _sample_tail_freePtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<context>, ffi.Pointer<token_data_array>,
              ffi.Float, ffi.Size)>>('llama_sample_tail_free');
  late final _sample_tail_free = _sample_tail_freePtr.asFunction<
      void Function(
          ffi.Pointer<context>, ffi.Pointer<token_data_array>, double, int)>();

  /// @details Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.
  void sample_typical(
    ffi.Pointer<context> ctx,
    ffi.Pointer<token_data_array> candidates,
    double p,
    int min_keep,
  ) {
    return _sample_typical(
      ctx,
      candidates,
      p,
      min_keep,
    );
  }

  late final _sample_typicalPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<context>, ffi.Pointer<token_data_array>,
              ffi.Float, ffi.Size)>>('llama_sample_typical');
  late final _sample_typical = _sample_typicalPtr.asFunction<
      void Function(
          ffi.Pointer<context>, ffi.Pointer<token_data_array>, double, int)>();

  void sample_temperature(
    ffi.Pointer<context> ctx,
    ffi.Pointer<token_data_array> candidates,
    double temp,
  ) {
    return _sample_temperature(
      ctx,
      candidates,
      temp,
    );
  }

  late final _sample_temperaturePtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<context>, ffi.Pointer<token_data_array>,
              ffi.Float)>>('llama_sample_temperature');
  late final _sample_temperature = _sample_temperaturePtr.asFunction<
      void Function(
          ffi.Pointer<context>, ffi.Pointer<token_data_array>, double)>();

  /// @details Apply constraints from grammar
  void sample_grammar(
    ffi.Pointer<context> ctx,
    ffi.Pointer<token_data_array> candidates,
    ffi.Pointer<grammar> grammar,
  ) {
    return _sample_grammar(
      ctx,
      candidates,
      grammar,
    );
  }

  late final _sample_grammarPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<context>, ffi.Pointer<token_data_array>,
              ffi.Pointer<grammar>)>>('llama_sample_grammar');
  late final _sample_grammar = _sample_grammarPtr.asFunction<
      void Function(ffi.Pointer<context>, ffi.Pointer<token_data_array>,
          ffi.Pointer<grammar>)>();

  /// @details Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
  /// @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
  /// @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
  /// @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
  /// @param m The number of tokens considered in the estimation of `s_hat`. This is an arbitrary value that is used to calculate `s_hat`, which in turn helps to calculate the value of `k`. In the paper, they use `m = 100`, but you can experiment with different values to see how it affects the performance of the algorithm.
  /// @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.
  int sample_token_mirostat(
    ffi.Pointer<context> ctx,
    ffi.Pointer<token_data_array> candidates,
    double tau,
    double eta,
    int m,
    ffi.Pointer<ffi.Float> mu,
  ) {
    return _sample_token_mirostat(
      ctx,
      candidates,
      tau,
      eta,
      m,
      mu,
    );
  }

  late final _sample_token_mirostatPtr = _lookup<
      ffi.NativeFunction<
          token Function(
              ffi.Pointer<context>,
              ffi.Pointer<token_data_array>,
              ffi.Float,
              ffi.Float,
              ffi.Int,
              ffi.Pointer<ffi.Float>)>>('llama_sample_token_mirostat');
  late final _sample_token_mirostat = _sample_token_mirostatPtr.asFunction<
      int Function(ffi.Pointer<context>, ffi.Pointer<token_data_array>, double,
          double, int, ffi.Pointer<ffi.Float>)>();

  /// @details Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
  /// @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
  /// @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
  /// @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
  /// @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.
  int sample_token_mirostat_v2(
    ffi.Pointer<context> ctx,
    ffi.Pointer<token_data_array> candidates,
    double tau,
    double eta,
    ffi.Pointer<ffi.Float> mu,
  ) {
    return _sample_token_mirostat_v2(
      ctx,
      candidates,
      tau,
      eta,
      mu,
    );
  }

  late final _sample_token_mirostat_v2Ptr = _lookup<
      ffi.NativeFunction<
          token Function(
              ffi.Pointer<context>,
              ffi.Pointer<token_data_array>,
              ffi.Float,
              ffi.Float,
              ffi.Pointer<ffi.Float>)>>('llama_sample_token_mirostat_v2');
  late final _sample_token_mirostat_v2 =
      _sample_token_mirostat_v2Ptr.asFunction<
          int Function(ffi.Pointer<context>, ffi.Pointer<token_data_array>,
              double, double, ffi.Pointer<ffi.Float>)>();

  /// @details Selects the token with the highest probability.
  int sample_token_greedy(
    ffi.Pointer<context> ctx,
    ffi.Pointer<token_data_array> candidates,
  ) {
    return _sample_token_greedy(
      ctx,
      candidates,
    );
  }

  late final _sample_token_greedyPtr = _lookup<
      ffi.NativeFunction<
          token Function(ffi.Pointer<context>,
              ffi.Pointer<token_data_array>)>>('llama_sample_token_greedy');
  late final _sample_token_greedy = _sample_token_greedyPtr.asFunction<
      int Function(ffi.Pointer<context>, ffi.Pointer<token_data_array>)>();

  /// @details Randomly selects a token from the candidates based on their probabilities.
  int sample_token(
    ffi.Pointer<context> ctx,
    ffi.Pointer<token_data_array> candidates,
  ) {
    return _sample_token(
      ctx,
      candidates,
    );
  }

  late final _sample_tokenPtr = _lookup<
      ffi.NativeFunction<
          token Function(ffi.Pointer<context>,
              ffi.Pointer<token_data_array>)>>('llama_sample_token');
  late final _sample_token = _sample_tokenPtr.asFunction<
      int Function(ffi.Pointer<context>, ffi.Pointer<token_data_array>)>();

  /// @details Accepts the sampled token into the grammar
  void grammar_accept_token(
    ffi.Pointer<context> ctx,
    ffi.Pointer<grammar> grammar,
    int token,
  ) {
    return _grammar_accept_token(
      ctx,
      grammar,
      token,
    );
  }

  late final _grammar_accept_tokenPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<context>, ffi.Pointer<grammar>,
              token)>>('llama_grammar_accept_token');
  late final _grammar_accept_token = _grammar_accept_tokenPtr.asFunction<
      void Function(ffi.Pointer<context>, ffi.Pointer<grammar>, int)>();

  /// @details Deterministically returns entire sentence constructed by a beam search.
  /// @param ctx Pointer to the llama_context.
  /// @param callback Invoked for each iteration of the beam_search loop, passing in beams_state.
  /// @param callback_data A pointer that is simply passed back to callback.
  /// @param n_beams Number of beams to use.
  /// @param n_past Number of tokens already evaluated.
  /// @param n_predict Maximum number of tokens to predict. EOS may occur earlier.
  /// @param n_threads Number of threads as passed to llama_eval().
  void beam_search(
    ffi.Pointer<context> ctx,
    beam_search_callback_fn_t callback,
    ffi.Pointer<ffi.Void> callback_data,
    int n_beams,
    int n_past,
    int n_predict,
    int n_threads,
  ) {
    return _beam_search(
      ctx,
      callback,
      callback_data,
      n_beams,
      n_past,
      n_predict,
      n_threads,
    );
  }

  late final _beam_searchPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<context>,
              beam_search_callback_fn_t,
              ffi.Pointer<ffi.Void>,
              ffi.Size,
              ffi.Int,
              ffi.Int,
              ffi.Int)>>('llama_beam_search');
  late final _beam_search = _beam_searchPtr.asFunction<
      void Function(ffi.Pointer<context>, beam_search_callback_fn_t,
          ffi.Pointer<ffi.Void>, int, int, int, int)>();

  /// Performance information
  timings get_timings(
    ffi.Pointer<context> ctx,
  ) {
    return _get_timings(
      ctx,
    );
  }

  late final _get_timingsPtr =
      _lookup<ffi.NativeFunction<timings Function(ffi.Pointer<context>)>>(
          'llama_get_timings');
  late final _get_timings =
      _get_timingsPtr.asFunction<timings Function(ffi.Pointer<context>)>();

  void print_timings(
    ffi.Pointer<context> ctx,
  ) {
    return _print_timings(
      ctx,
    );
  }

  late final _print_timingsPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<context>)>>(
          'llama_print_timings');
  late final _print_timings =
      _print_timingsPtr.asFunction<void Function(ffi.Pointer<context>)>();

  void reset_timings(
    ffi.Pointer<context> ctx,
  ) {
    return _reset_timings(
      ctx,
    );
  }

  late final _reset_timingsPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<context>)>>(
          'llama_reset_timings');
  late final _reset_timings =
      _reset_timingsPtr.asFunction<void Function(ffi.Pointer<context>)>();

  /// Print system information
  ffi.Pointer<ffi.Char> print_system_info() {
    return _print_system_info();
  }

  late final _print_system_infoPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function()>>(
          'llama_print_system_info');
  late final _print_system_info =
      _print_system_infoPtr.asFunction<ffi.Pointer<ffi.Char> Function()>();

  /// Set callback for all future logging events.
  /// If this is not called, or NULL is supplied, everything is output on stderr.
  void log_set(
    log_callback log_callback,
    ffi.Pointer<ffi.Void> user_data,
  ) {
    return _log_set(
      log_callback,
      user_data,
    );
  }

  late final _log_setPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              log_callback, ffi.Pointer<ffi.Void>)>>('llama_log_set');
  late final _log_set = _log_setPtr
      .asFunction<void Function(log_callback, ffi.Pointer<ffi.Void>)>();

  void dump_timing_info_yaml(
    ffi.Pointer<FILE> stream,
    ffi.Pointer<context> ctx,
  ) {
    return _dump_timing_info_yaml(
      stream,
      ctx,
    );
  }

  late final _dump_timing_info_yamlPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<FILE>,
              ffi.Pointer<context>)>>('llama_dump_timing_info_yaml');
  late final _dump_timing_info_yaml = _dump_timing_info_yamlPtr
      .asFunction<void Function(ffi.Pointer<FILE>, ffi.Pointer<context>)>();
}

/// C interface
///
/// TODO: show sample usage
final class model extends ffi.Opaque {}

final class context extends ffi.Opaque {}

abstract class log_level {
  static const int LOG_LEVEL_ERROR = 2;
  static const int LOG_LEVEL_WARN = 3;
  static const int LOG_LEVEL_INFO = 4;
}

abstract class vocab_type {
  /// SentencePiece
  static const int VOCAB_TYPE_SPM = 0;

  /// Byte Pair Encoding
  static const int VOCAB_TYPE_BPE = 1;
}

abstract class token_type {
  static const int TOKEN_TYPE_UNDEFINED = 0;
  static const int TOKEN_TYPE_NORMAL = 1;
  static const int TOKEN_TYPE_UNKNOWN = 2;
  static const int TOKEN_TYPE_CONTROL = 3;
  static const int TOKEN_TYPE_USER_DEFINED = 4;
  static const int TOKEN_TYPE_UNUSED = 5;
  static const int TOKEN_TYPE_BYTE = 6;
}

/// model file types
abstract class ftype {
  static const int FTYPE_ALL_F32 = 0;

  /// except 1d tensors
  static const int FTYPE_MOSTLY_F16 = 1;

  /// except 1d tensors
  static const int FTYPE_MOSTLY_Q4_0 = 2;

  /// except 1d tensors
  static const int FTYPE_MOSTLY_Q4_1 = 3;

  /// tok_embeddings.weight and output.weight are F16
  static const int FTYPE_MOSTLY_Q4_1_SOME_F16 = 4;

  /// except 1d tensors
  static const int FTYPE_MOSTLY_Q8_0 = 7;

  /// except 1d tensors
  static const int FTYPE_MOSTLY_Q5_0 = 8;

  /// except 1d tensors
  static const int FTYPE_MOSTLY_Q5_1 = 9;

  /// except 1d tensors
  static const int FTYPE_MOSTLY_Q2_K = 10;

  /// except 1d tensors
  static const int FTYPE_MOSTLY_Q3_K_S = 11;

  /// except 1d tensors
  static const int FTYPE_MOSTLY_Q3_K_M = 12;

  /// except 1d tensors
  static const int FTYPE_MOSTLY_Q3_K_L = 13;

  /// except 1d tensors
  static const int FTYPE_MOSTLY_Q4_K_S = 14;

  /// except 1d tensors
  static const int FTYPE_MOSTLY_Q4_K_M = 15;

  /// except 1d tensors
  static const int FTYPE_MOSTLY_Q5_K_S = 16;

  /// except 1d tensors
  static const int FTYPE_MOSTLY_Q5_K_M = 17;

  /// except 1d tensors
  static const int FTYPE_MOSTLY_Q6_K = 18;

  /// not specified in the model file
  static const int FTYPE_GUESSED = 1024;
}

final class token_data extends ffi.Struct {
  /// token id
  @token()
  external int id;

  /// log-odds of the token
  @ffi.Float()
  external double logit;

  /// probability of the token
  @ffi.Float()
  external double p;
}

typedef token = ffi.Int;

final class token_data_array extends ffi.Struct {
  external ffi.Pointer<token_data> data;

  @ffi.Size()
  external int size;

  @ffi.Bool()
  external bool sorted;
}

final class context_params extends ffi.Struct {
  /// RNG seed, -1 for random
  @ffi.Uint32()
  external int seed;

  /// text context
  @ffi.Int32()
  external int n_ctx;

  /// prompt processing batch size
  @ffi.Int32()
  external int n_batch;

  /// number of layers to store in VRAM
  @ffi.Int32()
  external int n_gpu_layers;

  /// the GPU that is used for scratch and small tensors
  @ffi.Int32()
  external int main_gpu;

  /// how to split layers across multiple GPUs (size: LLAMA_MAX_DEVICES)
  external ffi.Pointer<ffi.Float> tensor_split;

  /// RoPE base frequency
  @ffi.Float()
  external double rope_freq_base;

  /// RoPE frequency scaling factor
  @ffi.Float()
  external double rope_freq_scale;

  /// called with a progress value between 0 and 1, pass NULL to disable
  external progress_callback progress_callback1;

  /// context pointer passed to the progress callback
  external ffi.Pointer<ffi.Void> progress_callback_user_data;

  /// if true, reduce VRAM usage at the cost of performance
  @ffi.Bool()
  external bool low_vram;

  /// if true, use experimental mul_mat_q kernels
  @ffi.Bool()
  external bool mul_mat_q;

  /// use fp16 for KV cache
  @ffi.Bool()
  external bool f16_kv;

  /// the llama_eval() call computes all logits, not just the last one
  @ffi.Bool()
  external bool logits_all;

  /// only load the vocabulary, no weights
  @ffi.Bool()
  external bool vocab_only;

  /// use mmap if possible
  @ffi.Bool()
  external bool use_mmap;

  /// force system to keep model in RAM
  @ffi.Bool()
  external bool use_mlock;

  /// embedding mode only
  @ffi.Bool()
  external bool embedding;
}

typedef progress_callback = ffi.Pointer<
    ffi.NativeFunction<
        ffi.Void Function(ffi.Float progress, ffi.Pointer<ffi.Void> ctx)>>;

/// model quantization parameters
final class model_quantize_params extends ffi.Struct {
  /// number of threads to use for quantizing, if <=0 will use std::thread::hardware_concurrency()
  @ffi.Int()
  external int nthread;

  /// quantize to this llama_ftype
  @ffi.Int32()
  external int ftype;

  /// allow quantizing non-f32/f16 tensors
  @ffi.Bool()
  external bool allow_requantize;

  /// quantize output.weight
  @ffi.Bool()
  external bool quantize_output_tensor;

  /// only copy tensors - ftype, allow_requantize and quantize_output_tensor are ignored
  @ffi.Bool()
  external bool only_copy;
}

/// grammar types
final class grammar extends ffi.Opaque {}

/// grammar element type
abstract class gretype {
  /// end of rule definition
  static const int GRETYPE_END = 0;

  /// start of alternate definition for rule
  static const int GRETYPE_ALT = 1;

  /// non-terminal element: reference to rule
  static const int GRETYPE_RULE_REF = 2;

  /// terminal element: character (code point)
  static const int GRETYPE_CHAR = 3;

  /// inverse char(s) ([^a], [^a-b] [^abc])
  static const int GRETYPE_CHAR_NOT = 4;

  /// modifies a preceding LLAMA_GRETYPE_CHAR or LLAMA_GRETYPE_CHAR_ALT to
  /// be an inclusive range ([a-z])
  static const int GRETYPE_CHAR_RNG_UPPER = 5;

  /// modifies a preceding LLAMA_GRETYPE_CHAR or
  /// LLAMA_GRETYPE_CHAR_RNG_UPPER to add an alternate char to match ([ab], [a-zA])
  static const int GRETYPE_CHAR_ALT = 6;
}

final class grammar_element extends ffi.Struct {
  @ffi.Int32()
  external int type;

  /// Unicode code point or rule ID
  @ffi.Uint32()
  external int value;
}

/// performance timing information
final class timings extends ffi.Struct {
  @ffi.Double()
  external double t_start_ms;

  @ffi.Double()
  external double t_end_ms;

  @ffi.Double()
  external double t_load_ms;

  @ffi.Double()
  external double t_sample_ms;

  @ffi.Double()
  external double t_p_eval_ms;

  @ffi.Double()
  external double t_eval_ms;

  @ffi.Int32()
  external int n_sample;

  @ffi.Int32()
  external int n_p_eval;

  @ffi.Int32()
  external int n_eval;
}

/// Beam search
final class beam_view extends ffi.Struct {
  external ffi.Pointer<token> tokens;

  @ffi.Size()
  external int n_tokens;

  /// Cumulative beam probability (renormalized relative to all beams)
  @ffi.Float()
  external double p;

  /// Callback should set this to true when a beam is at end-of-beam.
  @ffi.Bool()
  external bool eob;
}

/// Passed to beam_search_callback function.
/// Whenever 0 < common_prefix_length, this number of tokens should be copied from any of the beams
/// (e.g. beams[0]) as they will be removed (shifted) from all beams in all subsequent callbacks.
/// These pointers are valid only during the synchronous callback, so should not be saved.
final class beams_state extends ffi.Struct {
  external ffi.Pointer<beam_view> beam_views;

  /// Number of elements in beam_views[].
  @ffi.Size()
  external int n_beams;

  /// Current max length of prefix tokens shared by all beams.
  @ffi.Size()
  external int common_prefix_length;

  /// True iff this is the last callback invocation.
  @ffi.Bool()
  external bool last_call;
}

/// Type of pointer to the beam_search_callback function.
/// void* callback_data is any custom data passed to llama_beam_search, that is subsequently
/// passed back to beam_search_callback. This avoids having to use global variables in the callback.
typedef beam_search_callback_fn_t = ffi.Pointer<
    ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ffi.Void>, beams_state)>>;

/// Signature for logging events
/// Note that text includes the new line character at the end for most events.
/// If your logging mechanism cannot handle that, check if the last character is '\n' and strip it
/// if it exists.
/// It might not exist for progress report where '.' is output repeatedly.
typedef log_callback = ffi.Pointer<
    ffi.NativeFunction<
        ffi.Void Function(ffi.Int32 level, ffi.Pointer<ffi.Char> text,
            ffi.Pointer<ffi.Void> user_data)>>;
typedef FILE = _IO_FILE;

final class _IO_FILE extends ffi.Struct {
  @ffi.Int()
  external int _flags;

  external ffi.Pointer<ffi.Char> _IO_read_ptr;

  external ffi.Pointer<ffi.Char> _IO_read_end;

  external ffi.Pointer<ffi.Char> _IO_read_base;

  external ffi.Pointer<ffi.Char> _IO_write_base;

  external ffi.Pointer<ffi.Char> _IO_write_ptr;

  external ffi.Pointer<ffi.Char> _IO_write_end;

  external ffi.Pointer<ffi.Char> _IO_buf_base;

  external ffi.Pointer<ffi.Char> _IO_buf_end;

  external ffi.Pointer<ffi.Char> _IO_save_base;

  external ffi.Pointer<ffi.Char> _IO_backup_base;

  external ffi.Pointer<ffi.Char> _IO_save_end;

  external ffi.Pointer<_IO_marker> _markers;

  external ffi.Pointer<_IO_FILE> _chain;

  @ffi.Int()
  external int _fileno;

  @ffi.Int()
  external int _flags2;

  @__off_t()
  external int _old_offset;

  @ffi.UnsignedShort()
  external int _cur_column;

  @ffi.SignedChar()
  external int _vtable_offset;

  @ffi.Array.multi([1])
  external ffi.Array<ffi.Char> _shortbuf;

  external ffi.Pointer<_IO_lock_t> _lock;

  @__off64_t()
  external int _offset;

  external ffi.Pointer<_IO_codecvt> _codecvt;

  external ffi.Pointer<_IO_wide_data> _wide_data;

  external ffi.Pointer<_IO_FILE> _freeres_list;

  external ffi.Pointer<ffi.Void> _freeres_buf;

  @ffi.Size()
  external int __pad5;

  @ffi.Int()
  external int _mode;

  @ffi.Array.multi([20])
  external ffi.Array<ffi.Char> _unused2;
}

final class _IO_marker extends ffi.Opaque {}

typedef __off_t = ffi.Long;
typedef _IO_lock_t = ffi.Void;
typedef __off64_t = ffi.Long;

final class _IO_codecvt extends ffi.Opaque {}

final class _IO_wide_data extends ffi.Opaque {}

const int MAX_DEVICES = 1;

const int DEFAULT_SEED = 4294967295;

const int FILE_MAGIC_GGSN = 1734833006;

const int SESSION_MAGIC = 1734833006;

const int SESSION_VERSION = 1;
